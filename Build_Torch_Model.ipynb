{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build Torch Model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g8zYG2lFrSPN",
        "7nJs3Xm6E1DC"
      ],
      "authorship_tag": "ABX9TyM2CqbabqDbNu2ee+70GVCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mario-td/Inference-for-Hand-Gesture-Classification/blob/master/Build_Torch_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JYDptYxAzfb"
      },
      "source": [
        "# **Load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84pJ5youlPu7"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSlHF1JM4LaC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOOubOgKhR93",
        "outputId": "b7e8a9d7-b29e-47d0-e2f2-c93dea698963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "# Path to the files\n",
        "path = '/content/drive/My Drive/MastersThesis/Dataset'\n",
        "\n",
        "training = pd.read_csv(path + '/TrainingSet2D.csv')\n",
        "test = pd.read_csv(path + '/TestSet2D.csv')\n",
        "\n",
        "# Data visualization\n",
        "training"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Sequence</th>\n",
              "      <th>j0_x</th>\n",
              "      <th>j0_y</th>\n",
              "      <th>j1_x</th>\n",
              "      <th>j1_y</th>\n",
              "      <th>j2_x</th>\n",
              "      <th>j2_y</th>\n",
              "      <th>j3_x</th>\n",
              "      <th>j3_y</th>\n",
              "      <th>j4_x</th>\n",
              "      <th>j4_y</th>\n",
              "      <th>j5_x</th>\n",
              "      <th>j5_y</th>\n",
              "      <th>j6_x</th>\n",
              "      <th>j6_y</th>\n",
              "      <th>j7_x</th>\n",
              "      <th>j7_y</th>\n",
              "      <th>j8_x</th>\n",
              "      <th>j8_y</th>\n",
              "      <th>j9_x</th>\n",
              "      <th>j9_y</th>\n",
              "      <th>j10_x</th>\n",
              "      <th>j10_y</th>\n",
              "      <th>j11_x</th>\n",
              "      <th>j11_y</th>\n",
              "      <th>j12_x</th>\n",
              "      <th>j12_y</th>\n",
              "      <th>j13_x</th>\n",
              "      <th>j13_y</th>\n",
              "      <th>j14_x</th>\n",
              "      <th>j14_y</th>\n",
              "      <th>j15_x</th>\n",
              "      <th>j15_y</th>\n",
              "      <th>j16_x</th>\n",
              "      <th>j16_y</th>\n",
              "      <th>j17_x</th>\n",
              "      <th>j17_y</th>\n",
              "      <th>j18_x</th>\n",
              "      <th>j18_y</th>\n",
              "      <th>j19_x</th>\n",
              "      <th>j19_y</th>\n",
              "      <th>j20_x</th>\n",
              "      <th>j20_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427588</td>\n",
              "      <td>0.782585</td>\n",
              "      <td>0.487573</td>\n",
              "      <td>0.679753</td>\n",
              "      <td>0.538989</td>\n",
              "      <td>0.599772</td>\n",
              "      <td>0.564697</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>0.607544</td>\n",
              "      <td>0.462663</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.474089</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.394108</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.348405</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.291276</td>\n",
              "      <td>0.410449</td>\n",
              "      <td>0.474089</td>\n",
              "      <td>0.401880</td>\n",
              "      <td>0.405534</td>\n",
              "      <td>0.401880</td>\n",
              "      <td>0.336979</td>\n",
              "      <td>0.393311</td>\n",
              "      <td>0.268424</td>\n",
              "      <td>0.367603</td>\n",
              "      <td>0.496940</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.439811</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.371257</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.314128</td>\n",
              "      <td>0.341895</td>\n",
              "      <td>0.565495</td>\n",
              "      <td>0.307617</td>\n",
              "      <td>0.519792</td>\n",
              "      <td>0.299048</td>\n",
              "      <td>0.474089</td>\n",
              "      <td>0.307617</td>\n",
              "      <td>0.416960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.376929</td>\n",
              "      <td>0.779590</td>\n",
              "      <td>0.444702</td>\n",
              "      <td>0.666634</td>\n",
              "      <td>0.470117</td>\n",
              "      <td>0.553678</td>\n",
              "      <td>0.487061</td>\n",
              "      <td>0.474609</td>\n",
              "      <td>0.520947</td>\n",
              "      <td>0.429427</td>\n",
              "      <td>0.368457</td>\n",
              "      <td>0.452018</td>\n",
              "      <td>0.368457</td>\n",
              "      <td>0.384245</td>\n",
              "      <td>0.359985</td>\n",
              "      <td>0.339062</td>\n",
              "      <td>0.351514</td>\n",
              "      <td>0.282585</td>\n",
              "      <td>0.326099</td>\n",
              "      <td>0.474609</td>\n",
              "      <td>0.317627</td>\n",
              "      <td>0.395540</td>\n",
              "      <td>0.300684</td>\n",
              "      <td>0.327767</td>\n",
              "      <td>0.283740</td>\n",
              "      <td>0.271289</td>\n",
              "      <td>0.292212</td>\n",
              "      <td>0.508496</td>\n",
              "      <td>0.266797</td>\n",
              "      <td>0.429427</td>\n",
              "      <td>0.249854</td>\n",
              "      <td>0.372949</td>\n",
              "      <td>0.249854</td>\n",
              "      <td>0.316471</td>\n",
              "      <td>0.275269</td>\n",
              "      <td>0.564974</td>\n",
              "      <td>0.215967</td>\n",
              "      <td>0.519792</td>\n",
              "      <td>0.207495</td>\n",
              "      <td>0.474609</td>\n",
              "      <td>0.207495</td>\n",
              "      <td>0.440723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.364795</td>\n",
              "      <td>0.769401</td>\n",
              "      <td>0.416064</td>\n",
              "      <td>0.666862</td>\n",
              "      <td>0.450244</td>\n",
              "      <td>0.552930</td>\n",
              "      <td>0.467334</td>\n",
              "      <td>0.473177</td>\n",
              "      <td>0.492969</td>\n",
              "      <td>0.427604</td>\n",
              "      <td>0.347705</td>\n",
              "      <td>0.450391</td>\n",
              "      <td>0.339160</td>\n",
              "      <td>0.393424</td>\n",
              "      <td>0.330615</td>\n",
              "      <td>0.336458</td>\n",
              "      <td>0.313525</td>\n",
              "      <td>0.279492</td>\n",
              "      <td>0.304980</td>\n",
              "      <td>0.473177</td>\n",
              "      <td>0.296436</td>\n",
              "      <td>0.393424</td>\n",
              "      <td>0.270801</td>\n",
              "      <td>0.336458</td>\n",
              "      <td>0.253711</td>\n",
              "      <td>0.268099</td>\n",
              "      <td>0.270801</td>\n",
              "      <td>0.507357</td>\n",
              "      <td>0.236621</td>\n",
              "      <td>0.438997</td>\n",
              "      <td>0.219531</td>\n",
              "      <td>0.382031</td>\n",
              "      <td>0.210986</td>\n",
              "      <td>0.325065</td>\n",
              "      <td>0.253711</td>\n",
              "      <td>0.564323</td>\n",
              "      <td>0.202441</td>\n",
              "      <td>0.530143</td>\n",
              "      <td>0.185352</td>\n",
              "      <td>0.484570</td>\n",
              "      <td>0.176807</td>\n",
              "      <td>0.438997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.349219</td>\n",
              "      <td>0.747461</td>\n",
              "      <td>0.400049</td>\n",
              "      <td>0.657096</td>\n",
              "      <td>0.442407</td>\n",
              "      <td>0.555436</td>\n",
              "      <td>0.467822</td>\n",
              "      <td>0.476367</td>\n",
              "      <td>0.484766</td>\n",
              "      <td>0.408594</td>\n",
              "      <td>0.340747</td>\n",
              "      <td>0.465072</td>\n",
              "      <td>0.323804</td>\n",
              "      <td>0.386003</td>\n",
              "      <td>0.306860</td>\n",
              "      <td>0.329525</td>\n",
              "      <td>0.289917</td>\n",
              "      <td>0.284342</td>\n",
              "      <td>0.289917</td>\n",
              "      <td>0.476367</td>\n",
              "      <td>0.264502</td>\n",
              "      <td>0.397298</td>\n",
              "      <td>0.247559</td>\n",
              "      <td>0.329525</td>\n",
              "      <td>0.222144</td>\n",
              "      <td>0.273047</td>\n",
              "      <td>0.256030</td>\n",
              "      <td>0.510254</td>\n",
              "      <td>0.222144</td>\n",
              "      <td>0.431185</td>\n",
              "      <td>0.205200</td>\n",
              "      <td>0.374707</td>\n",
              "      <td>0.196729</td>\n",
              "      <td>0.329525</td>\n",
              "      <td>0.239087</td>\n",
              "      <td>0.566732</td>\n",
              "      <td>0.188257</td>\n",
              "      <td>0.532845</td>\n",
              "      <td>0.171313</td>\n",
              "      <td>0.498958</td>\n",
              "      <td>0.162842</td>\n",
              "      <td>0.453776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.342187</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.393750</td>\n",
              "      <td>0.646875</td>\n",
              "      <td>0.428125</td>\n",
              "      <td>0.555208</td>\n",
              "      <td>0.436719</td>\n",
              "      <td>0.463542</td>\n",
              "      <td>0.453906</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.316406</td>\n",
              "      <td>0.463542</td>\n",
              "      <td>0.299219</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.282031</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.264844</td>\n",
              "      <td>0.280208</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.247656</td>\n",
              "      <td>0.394792</td>\n",
              "      <td>0.221875</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.196094</td>\n",
              "      <td>0.280208</td>\n",
              "      <td>0.239063</td>\n",
              "      <td>0.509375</td>\n",
              "      <td>0.204687</td>\n",
              "      <td>0.440625</td>\n",
              "      <td>0.178906</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.161719</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.221875</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.178906</td>\n",
              "      <td>0.532292</td>\n",
              "      <td>0.153125</td>\n",
              "      <td>0.497917</td>\n",
              "      <td>0.135937</td>\n",
              "      <td>0.452083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111547</th>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>0.594629</td>\n",
              "      <td>0.701237</td>\n",
              "      <td>0.634863</td>\n",
              "      <td>0.687826</td>\n",
              "      <td>0.670068</td>\n",
              "      <td>0.654297</td>\n",
              "      <td>0.685156</td>\n",
              "      <td>0.620768</td>\n",
              "      <td>0.675098</td>\n",
              "      <td>0.587240</td>\n",
              "      <td>0.624805</td>\n",
              "      <td>0.526888</td>\n",
              "      <td>0.619775</td>\n",
              "      <td>0.493359</td>\n",
              "      <td>0.624805</td>\n",
              "      <td>0.466536</td>\n",
              "      <td>0.609717</td>\n",
              "      <td>0.439714</td>\n",
              "      <td>0.584570</td>\n",
              "      <td>0.526888</td>\n",
              "      <td>0.579541</td>\n",
              "      <td>0.493359</td>\n",
              "      <td>0.569482</td>\n",
              "      <td>0.479948</td>\n",
              "      <td>0.559424</td>\n",
              "      <td>0.446419</td>\n",
              "      <td>0.564453</td>\n",
              "      <td>0.547005</td>\n",
              "      <td>0.544336</td>\n",
              "      <td>0.526888</td>\n",
              "      <td>0.534277</td>\n",
              "      <td>0.506771</td>\n",
              "      <td>0.524219</td>\n",
              "      <td>0.506771</td>\n",
              "      <td>0.529248</td>\n",
              "      <td>0.587240</td>\n",
              "      <td>0.524219</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.519189</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.519189</td>\n",
              "      <td>0.560417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111548</th>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>0.605029</td>\n",
              "      <td>0.717969</td>\n",
              "      <td>0.634570</td>\n",
              "      <td>0.686458</td>\n",
              "      <td>0.670020</td>\n",
              "      <td>0.662826</td>\n",
              "      <td>0.681836</td>\n",
              "      <td>0.623437</td>\n",
              "      <td>0.687744</td>\n",
              "      <td>0.599805</td>\n",
              "      <td>0.616846</td>\n",
              "      <td>0.536784</td>\n",
              "      <td>0.616846</td>\n",
              "      <td>0.481641</td>\n",
              "      <td>0.616846</td>\n",
              "      <td>0.450130</td>\n",
              "      <td>0.610938</td>\n",
              "      <td>0.410742</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>0.528906</td>\n",
              "      <td>0.569580</td>\n",
              "      <td>0.473763</td>\n",
              "      <td>0.557764</td>\n",
              "      <td>0.434375</td>\n",
              "      <td>0.551855</td>\n",
              "      <td>0.402865</td>\n",
              "      <td>0.551855</td>\n",
              "      <td>0.552539</td>\n",
              "      <td>0.534131</td>\n",
              "      <td>0.513151</td>\n",
              "      <td>0.522314</td>\n",
              "      <td>0.489518</td>\n",
              "      <td>0.504590</td>\n",
              "      <td>0.458008</td>\n",
              "      <td>0.528223</td>\n",
              "      <td>0.584049</td>\n",
              "      <td>0.516406</td>\n",
              "      <td>0.568294</td>\n",
              "      <td>0.498682</td>\n",
              "      <td>0.552539</td>\n",
              "      <td>0.492773</td>\n",
              "      <td>0.528906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111549</th>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>0.602441</td>\n",
              "      <td>0.715365</td>\n",
              "      <td>0.631494</td>\n",
              "      <td>0.699870</td>\n",
              "      <td>0.666357</td>\n",
              "      <td>0.668880</td>\n",
              "      <td>0.677979</td>\n",
              "      <td>0.637891</td>\n",
              "      <td>0.689600</td>\n",
              "      <td>0.606901</td>\n",
              "      <td>0.619873</td>\n",
              "      <td>0.537174</td>\n",
              "      <td>0.619873</td>\n",
              "      <td>0.475195</td>\n",
              "      <td>0.619873</td>\n",
              "      <td>0.444206</td>\n",
              "      <td>0.614062</td>\n",
              "      <td>0.405469</td>\n",
              "      <td>0.579199</td>\n",
              "      <td>0.529427</td>\n",
              "      <td>0.567578</td>\n",
              "      <td>0.475195</td>\n",
              "      <td>0.555957</td>\n",
              "      <td>0.428711</td>\n",
              "      <td>0.550146</td>\n",
              "      <td>0.397721</td>\n",
              "      <td>0.550146</td>\n",
              "      <td>0.552669</td>\n",
              "      <td>0.532715</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.515283</td>\n",
              "      <td>0.475195</td>\n",
              "      <td>0.503662</td>\n",
              "      <td>0.451953</td>\n",
              "      <td>0.526904</td>\n",
              "      <td>0.583659</td>\n",
              "      <td>0.509473</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.492041</td>\n",
              "      <td>0.544922</td>\n",
              "      <td>0.486230</td>\n",
              "      <td>0.521680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111550</th>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>0.601611</td>\n",
              "      <td>0.717057</td>\n",
              "      <td>0.630786</td>\n",
              "      <td>0.693717</td>\n",
              "      <td>0.665796</td>\n",
              "      <td>0.670378</td>\n",
              "      <td>0.683301</td>\n",
              "      <td>0.639258</td>\n",
              "      <td>0.689136</td>\n",
              "      <td>0.608138</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.538118</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.491439</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.444759</td>\n",
              "      <td>0.613281</td>\n",
              "      <td>0.405859</td>\n",
              "      <td>0.578271</td>\n",
              "      <td>0.530339</td>\n",
              "      <td>0.572437</td>\n",
              "      <td>0.475879</td>\n",
              "      <td>0.560767</td>\n",
              "      <td>0.436979</td>\n",
              "      <td>0.549097</td>\n",
              "      <td>0.398079</td>\n",
              "      <td>0.549097</td>\n",
              "      <td>0.553678</td>\n",
              "      <td>0.537427</td>\n",
              "      <td>0.506999</td>\n",
              "      <td>0.525757</td>\n",
              "      <td>0.475879</td>\n",
              "      <td>0.508252</td>\n",
              "      <td>0.444759</td>\n",
              "      <td>0.525757</td>\n",
              "      <td>0.592578</td>\n",
              "      <td>0.508252</td>\n",
              "      <td>0.561458</td>\n",
              "      <td>0.490747</td>\n",
              "      <td>0.538118</td>\n",
              "      <td>0.479077</td>\n",
              "      <td>0.514779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111551</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.596777</td>\n",
              "      <td>0.721745</td>\n",
              "      <td>0.625586</td>\n",
              "      <td>0.698698</td>\n",
              "      <td>0.665918</td>\n",
              "      <td>0.675651</td>\n",
              "      <td>0.677441</td>\n",
              "      <td>0.637240</td>\n",
              "      <td>0.688965</td>\n",
              "      <td>0.606510</td>\n",
              "      <td>0.619824</td>\n",
              "      <td>0.545052</td>\n",
              "      <td>0.619824</td>\n",
              "      <td>0.491276</td>\n",
              "      <td>0.619824</td>\n",
              "      <td>0.452865</td>\n",
              "      <td>0.614062</td>\n",
              "      <td>0.414453</td>\n",
              "      <td>0.585254</td>\n",
              "      <td>0.529687</td>\n",
              "      <td>0.573730</td>\n",
              "      <td>0.475911</td>\n",
              "      <td>0.562207</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.556445</td>\n",
              "      <td>0.399089</td>\n",
              "      <td>0.550684</td>\n",
              "      <td>0.552734</td>\n",
              "      <td>0.539160</td>\n",
              "      <td>0.514323</td>\n",
              "      <td>0.527637</td>\n",
              "      <td>0.475911</td>\n",
              "      <td>0.516113</td>\n",
              "      <td>0.445182</td>\n",
              "      <td>0.527637</td>\n",
              "      <td>0.591146</td>\n",
              "      <td>0.510352</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.493066</td>\n",
              "      <td>0.537370</td>\n",
              "      <td>0.481543</td>\n",
              "      <td>0.514323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>111552 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Label  Sequence      j0_x  ...     j19_y     j20_x     j20_y\n",
              "0           0         0  0.427588  ...  0.474089  0.307617  0.416960\n",
              "1           0         1  0.376929  ...  0.474609  0.207495  0.440723\n",
              "2           0         2  0.364795  ...  0.484570  0.176807  0.438997\n",
              "3           0         3  0.349219  ...  0.498958  0.162842  0.453776\n",
              "4           0         4  0.342187  ...  0.497917  0.135937  0.452083\n",
              "...       ...       ...       ...  ...       ...       ...       ...\n",
              "111547      4        27  0.594629  ...  0.560417  0.519189  0.560417\n",
              "111548      4        28  0.605029  ...  0.552539  0.492773  0.528906\n",
              "111549      4        29  0.602441  ...  0.544922  0.486230  0.521680\n",
              "111550      4        30  0.601611  ...  0.538118  0.479077  0.514779\n",
              "111551      4        31  0.596777  ...  0.537370  0.481543  0.514323\n",
              "\n",
              "[111552 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-zAt-V0L3fw"
      },
      "source": [
        "n_classes = len(training['Label'].unique())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnRoqinhq0F",
        "outputId": "5c46150e-7ec7-45f7-a1d2-8b85973bc4e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the number of frames per sample\n",
        "t_steps = max(training['Sequence']) + 1\n",
        "t_steps"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSdg6PSbhyXE"
      },
      "source": [
        "# Get the labels\n",
        "Y_train = np.array(training['Label'][0::t_steps][:], dtype=np.int32)\n",
        "Y_test = np.array(test['Label'][0::t_steps][:], dtype=np.int32)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0842ivyhyeL"
      },
      "source": [
        "# Get the data\n",
        "X_train = pd.DataFrame.to_numpy(training[training.columns[2:]])\n",
        "X_test = pd.DataFrame.to_numpy(test[test.columns[2:]])\n",
        "# Split into the number of samples\n",
        "X_train = np.array(np.split(X_train, Y_train.size))\n",
        "X_test = np.array(np.split(X_test, Y_test.size))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m2jTkBPhyji"
      },
      "source": [
        "# Split into train, validation \n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcMIch7mE1M",
        "outputId": "b48c83b9-bed7-4cba-c849-6bcdf32256eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('X_train shape '+ str(X_train.shape))\n",
        "print('X_val shape '+ str(X_val.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape (3137, 32, 42)\n",
            "X_val shape (349, 32, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aXj-yR0igwD"
      },
      "source": [
        "# **Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgyjClybkdn-"
      },
      "source": [
        "def dataAugmentation(X_, Y, c):\n",
        "    X = X_[Y == c]\n",
        "    # Duplicate the samples by inverting the order of the sequence in time\n",
        "    X_inv_t = [X[i][::-1] for i in range(len(X))]\n",
        "    X = np.append(X, X_inv_t, 0)\n",
        "    # Duplicate the samples by inverting the x coordinate\n",
        "    # with respect to the y axis (mirroring)\n",
        "    X_inv_pos = []\n",
        "    for k in range(len(X)):\n",
        "        Xj = []\n",
        "        for j in range(len(X[k])):\n",
        "            Xi = []\n",
        "            for i in range(len(X[k][j])):\n",
        "                if i % 3 == 0:\n",
        "                    Xi.append(abs(X[k][j][i] - 1))\n",
        "                else:\n",
        "                    Xi.append(X[k][j][i])\n",
        "            Xj.append(Xi)\n",
        "        X_inv_pos.append(Xj)\n",
        "    X = np.append(X, X_inv_pos, 0)\n",
        "    X_ = np.append(X_[Y != c], X, 0)\n",
        "    Y = np.append(Y[Y != c], np.ones(len(X)) * c)\n",
        "    return X_, Y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvCD0tbtifqq"
      },
      "source": [
        "# Duplicate samples of all classes in training, validation and test sets\n",
        "# separately to avoid overfitting mixing samples\n",
        "for c in range(n_classes):\n",
        "    X_train, Y_train = dataAugmentation(X_train, Y_train, c)\n",
        "    X_val, Y_val = dataAugmentation(X_val, Y_val, c)\n",
        "    X_test, Y_test = dataAugmentation(X_test, Y_test, c)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY9GJc1lpI1w",
        "outputId": "006c3d4d-152e-4732-c9dd-919a54fa6ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prints the number of samples per class in the training and test set\n",
        "for i in range(n_classes):\n",
        "    print('train samples %d: ' % i + str(np.count_nonzero(Y_train == i)))\n",
        "print('----------------------')\n",
        "for i in range(n_classes):\n",
        "    print('val samples %d: ' % i + str(np.count_nonzero(Y_val == i)))\n",
        "print('----------------------')\n",
        "for i in range(n_classes):\n",
        "    print('test samples %d: ' % i + str(np.count_nonzero(Y_test == i)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train samples 0: 2524\n",
            "train samples 1: 2432\n",
            "train samples 2: 2480\n",
            "train samples 3: 2464\n",
            "train samples 4: 2648\n",
            "----------------------\n",
            "val samples 0: 324\n",
            "val samples 1: 276\n",
            "val samples 2: 268\n",
            "val samples 3: 224\n",
            "val samples 4: 304\n",
            "----------------------\n",
            "test samples 0: 288\n",
            "test samples 1: 284\n",
            "test samples 2: 300\n",
            "test samples 3: 288\n",
            "test samples 4: 288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0I9Yz2LAAqS"
      },
      "source": [
        "# Convert to tensor\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_val = torch.from_numpy(X_val)\n",
        "X_test = torch.from_numpy(X_test) \n",
        "Y_train = torch.from_numpy(Y_train)\n",
        "Y_val = torch.from_numpy(Y_val)\n",
        "Y_test = torch.from_numpy(Y_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUNuVbCyUTRB"
      },
      "source": [
        "X_train = X_train.transpose(1,2)\n",
        "X_val = X_val.transpose(1,2)\n",
        "X_test = X_test.transpose(1,2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9iJtretAyE7"
      },
      "source": [
        "# **Build Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvNu26noqu9R",
        "outputId": "8a2b11c0-4546-43c6-8b22-304c44cf4097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_inputs = X_train[0].shape[0]\n",
        "n_filters_conv1 = 64\n",
        "n_filters_conv2 = 64\n",
        "hidden_lstm = 200\n",
        "n_fc = 5600\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.conv1 = nn.Conv1d(in_channels=n_inputs, out_channels=n_filters_conv1, kernel_size=3, stride=1)\n",
        "      self.conv2 = nn.Conv1d(in_channels=n_filters_conv1, out_channels=n_filters_conv2, kernel_size=3, stride=1)\n",
        "      self.flatten = nn.Flatten(start_dim=1, end_dim=2)\n",
        "      self.lstm1 = torch.nn.LSTM(\n",
        "            input_size= n_filters_conv2,\n",
        "            hidden_size=hidden_lstm,\n",
        "            batch_first=True,\n",
        "            num_layers=1)\n",
        "      self.lin = nn.Linear(n_fc, n_classes)\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.softmax = nn.LogSoftmax(dim = 1)\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = x.view(-1, 28, n_filters_conv2)\n",
        "      x, _ = self.lstm1(x)\n",
        "      x = x.contiguous().view(-1, n_fc)\n",
        "      x = self.lin(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv1d(42, 64, kernel_size=(3,), stride=(1,))\n",
            "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
            "  (flatten): Flatten(start_dim=1, end_dim=2)\n",
            "  (lstm1): LSTM(64, 200, batch_first=True)\n",
            "  (lin): Linear(in_features=5600, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHEEqUrnqFgt"
      },
      "source": [
        "learning_rate = 0.0011\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "crossentropy = nn.CrossEntropyLoss()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHmqu3g6Ak26"
      },
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Parameters\n",
        "params = {'batch_size': 1064,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ZEbwE4wgCo"
      },
      "source": [
        "train_data = []\n",
        "for i in range(len(X_train)):\n",
        "   train_data.append([X_train[i], Y_train[i]])\n",
        "val_data = []\n",
        "for i in range(len(X_val)):\n",
        "   val_data.append([X_val[i], Y_val[i]])\n",
        "test_data = []\n",
        "for i in range(len(X_test)):\n",
        "   test_data.append([X_test[i], Y_test[i]])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, **params)\n",
        "validationloader = torch.utils.data.DataLoader(val_data,  **params)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=test.shape[0])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSP15MaECTrK",
        "outputId": "d9491526-3d62-4f51-918a-a734b746c04d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(150):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        local_batch, local_labels = data\n",
        "        # Transfer to GPU\n",
        "        # local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "        # Switch model to training mode, clear gradient accumulators\n",
        "        net.train()\n",
        "        optimizer.zero_grad()\n",
        "        # Forward + Backward + Optimize\n",
        "        outputs = net(local_batch.float())\n",
        "        loss = crossentropy(outputs, local_labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy of predictions in the current batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += local_labels.size(0)\n",
        "        correct += (predicted == local_labels.long()).sum().item()\n",
        "    # Print statistics\n",
        "    print(\n",
        "        'Epoch %d:\\nloss: %.3f, accuracy: %.3f' %\n",
        "        (epoch + 1, loss.item(), 100 * correct/total))\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for local_batch, local_labels in validationloader:\n",
        "            # Transfer to GPU\n",
        "            # local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "            net.eval()\n",
        "            outputs = net(local_batch.float())\n",
        "            loss = crossentropy(outputs, local_labels.long())\n",
        "            # Calculate accuracy of predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += local_labels.size(0)\n",
        "            correct += (predicted == local_labels.long()).sum().item()\n",
        "        print(\n",
        "            'val_loss: %.3f, val_accuracy: %.3f' %\n",
        "            (loss.item(), 100 * correct/total))\n",
        "    print('-------------------------------------')\n",
        "print('Finished Training')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1:\n",
            "loss: 1.537, accuracy: 24.657\n",
            "val_loss: 1.515, val_accuracy: 45.272\n",
            "-------------------------------------\n",
            "Epoch 2:\n",
            "loss: 1.367, accuracy: 43.840\n",
            "val_loss: 1.025, val_accuracy: 56.948\n",
            "-------------------------------------\n",
            "Epoch 3:\n",
            "loss: 1.156, accuracy: 45.067\n",
            "val_loss: 1.043, val_accuracy: 58.166\n",
            "-------------------------------------\n",
            "Epoch 4:\n",
            "loss: 0.936, accuracy: 55.969\n",
            "val_loss: 0.826, val_accuracy: 66.261\n",
            "-------------------------------------\n",
            "Epoch 5:\n",
            "loss: 0.809, accuracy: 60.751\n",
            "val_loss: 0.737, val_accuracy: 66.046\n",
            "-------------------------------------\n",
            "Epoch 6:\n",
            "loss: 0.797, accuracy: 64.600\n",
            "val_loss: 0.683, val_accuracy: 67.837\n",
            "-------------------------------------\n",
            "Epoch 7:\n",
            "loss: 0.770, accuracy: 65.787\n",
            "val_loss: 0.596, val_accuracy: 71.848\n",
            "-------------------------------------\n",
            "Epoch 8:\n",
            "loss: 0.675, accuracy: 68.911\n",
            "val_loss: 0.600, val_accuracy: 72.636\n",
            "-------------------------------------\n",
            "Epoch 9:\n",
            "loss: 0.617, accuracy: 72.235\n",
            "val_loss: 0.500, val_accuracy: 78.940\n",
            "-------------------------------------\n",
            "Epoch 10:\n",
            "loss: 0.651, accuracy: 71.398\n",
            "val_loss: 0.663, val_accuracy: 69.413\n",
            "-------------------------------------\n",
            "Epoch 11:\n",
            "loss: 0.614, accuracy: 73.382\n",
            "val_loss: 0.464, val_accuracy: 80.802\n",
            "-------------------------------------\n",
            "Epoch 12:\n",
            "loss: 0.636, accuracy: 75.056\n",
            "val_loss: 0.700, val_accuracy: 76.146\n",
            "-------------------------------------\n",
            "Epoch 13:\n",
            "loss: 0.612, accuracy: 73.717\n",
            "val_loss: 0.503, val_accuracy: 80.874\n",
            "-------------------------------------\n",
            "Epoch 14:\n",
            "loss: 0.856, accuracy: 75.566\n",
            "val_loss: 0.951, val_accuracy: 67.980\n",
            "-------------------------------------\n",
            "Epoch 15:\n",
            "loss: 0.659, accuracy: 70.027\n",
            "val_loss: 0.544, val_accuracy: 78.653\n",
            "-------------------------------------\n",
            "Epoch 16:\n",
            "loss: 0.575, accuracy: 75.685\n",
            "val_loss: 0.500, val_accuracy: 81.447\n",
            "-------------------------------------\n",
            "Epoch 17:\n",
            "loss: 0.524, accuracy: 77.088\n",
            "val_loss: 0.561, val_accuracy: 77.221\n",
            "-------------------------------------\n",
            "Epoch 18:\n",
            "loss: 0.493, accuracy: 76.745\n",
            "val_loss: 0.499, val_accuracy: 82.163\n",
            "-------------------------------------\n",
            "Epoch 19:\n",
            "loss: 0.473, accuracy: 80.268\n",
            "val_loss: 0.364, val_accuracy: 85.602\n",
            "-------------------------------------\n",
            "Epoch 20:\n",
            "loss: 0.411, accuracy: 83.296\n",
            "val_loss: 0.375, val_accuracy: 84.456\n",
            "-------------------------------------\n",
            "Epoch 21:\n",
            "loss: 0.531, accuracy: 83.352\n",
            "val_loss: 0.381, val_accuracy: 83.811\n",
            "-------------------------------------\n",
            "Epoch 22:\n",
            "loss: 0.494, accuracy: 77.869\n",
            "val_loss: 0.426, val_accuracy: 82.020\n",
            "-------------------------------------\n",
            "Epoch 23:\n",
            "loss: 0.421, accuracy: 81.997\n",
            "val_loss: 0.427, val_accuracy: 85.673\n",
            "-------------------------------------\n",
            "Epoch 24:\n",
            "loss: 0.451, accuracy: 83.081\n",
            "val_loss: 0.486, val_accuracy: 82.163\n",
            "-------------------------------------\n",
            "Epoch 25:\n",
            "loss: 0.407, accuracy: 83.734\n",
            "val_loss: 0.337, val_accuracy: 86.819\n",
            "-------------------------------------\n",
            "Epoch 26:\n",
            "loss: 0.336, accuracy: 86.819\n",
            "val_loss: 0.322, val_accuracy: 88.395\n",
            "-------------------------------------\n",
            "Epoch 27:\n",
            "loss: 0.556, accuracy: 80.674\n",
            "val_loss: 0.459, val_accuracy: 79.370\n",
            "-------------------------------------\n",
            "Epoch 28:\n",
            "loss: 0.476, accuracy: 78.148\n",
            "val_loss: 0.447, val_accuracy: 84.384\n",
            "-------------------------------------\n",
            "Epoch 29:\n",
            "loss: 0.385, accuracy: 83.902\n",
            "val_loss: 0.348, val_accuracy: 86.533\n",
            "-------------------------------------\n",
            "Epoch 30:\n",
            "loss: 0.358, accuracy: 85.607\n",
            "val_loss: 0.340, val_accuracy: 89.183\n",
            "-------------------------------------\n",
            "Epoch 31:\n",
            "loss: 0.333, accuracy: 87.233\n",
            "val_loss: 0.326, val_accuracy: 88.610\n",
            "-------------------------------------\n",
            "Epoch 32:\n",
            "loss: 0.342, accuracy: 87.902\n",
            "val_loss: 0.353, val_accuracy: 88.539\n",
            "-------------------------------------\n",
            "Epoch 33:\n",
            "loss: 0.265, accuracy: 89.321\n",
            "val_loss: 0.277, val_accuracy: 87.894\n",
            "-------------------------------------\n",
            "Epoch 34:\n",
            "loss: 0.302, accuracy: 89.026\n",
            "val_loss: 0.266, val_accuracy: 90.043\n",
            "-------------------------------------\n",
            "Epoch 35:\n",
            "loss: 0.321, accuracy: 88.978\n",
            "val_loss: 0.266, val_accuracy: 92.908\n",
            "-------------------------------------\n",
            "Epoch 36:\n",
            "loss: 0.324, accuracy: 88.612\n",
            "val_loss: 0.281, val_accuracy: 91.619\n",
            "-------------------------------------\n",
            "Epoch 37:\n",
            "loss: 0.232, accuracy: 90.245\n",
            "val_loss: 0.262, val_accuracy: 92.479\n",
            "-------------------------------------\n",
            "Epoch 38:\n",
            "loss: 0.226, accuracy: 91.696\n",
            "val_loss: 0.197, val_accuracy: 93.840\n",
            "-------------------------------------\n",
            "Epoch 39:\n",
            "loss: 0.222, accuracy: 92.405\n",
            "val_loss: 0.206, val_accuracy: 93.052\n",
            "-------------------------------------\n",
            "Epoch 40:\n",
            "loss: 0.186, accuracy: 91.704\n",
            "val_loss: 0.230, val_accuracy: 91.834\n",
            "-------------------------------------\n",
            "Epoch 41:\n",
            "loss: 0.273, accuracy: 89.990\n",
            "val_loss: 0.471, val_accuracy: 82.020\n",
            "-------------------------------------\n",
            "Epoch 42:\n",
            "loss: 0.713, accuracy: 77.494\n",
            "val_loss: 0.459, val_accuracy: 82.235\n",
            "-------------------------------------\n",
            "Epoch 43:\n",
            "loss: 0.349, accuracy: 82.786\n",
            "val_loss: 0.335, val_accuracy: 90.330\n",
            "-------------------------------------\n",
            "Epoch 44:\n",
            "loss: 0.290, accuracy: 88.731\n",
            "val_loss: 0.209, val_accuracy: 92.049\n",
            "-------------------------------------\n",
            "Epoch 45:\n",
            "loss: 0.223, accuracy: 91.513\n",
            "val_loss: 0.210, val_accuracy: 94.484\n",
            "-------------------------------------\n",
            "Epoch 46:\n",
            "loss: 0.222, accuracy: 92.851\n",
            "val_loss: 0.184, val_accuracy: 93.983\n",
            "-------------------------------------\n",
            "Epoch 47:\n",
            "loss: 0.193, accuracy: 92.899\n",
            "val_loss: 0.179, val_accuracy: 93.983\n",
            "-------------------------------------\n",
            "Epoch 48:\n",
            "loss: 0.212, accuracy: 93.401\n",
            "val_loss: 0.179, val_accuracy: 93.911\n",
            "-------------------------------------\n",
            "Epoch 49:\n",
            "loss: 0.164, accuracy: 93.059\n",
            "val_loss: 0.176, val_accuracy: 94.341\n",
            "-------------------------------------\n",
            "Epoch 50:\n",
            "loss: 0.168, accuracy: 93.640\n",
            "val_loss: 0.176, val_accuracy: 94.699\n",
            "-------------------------------------\n",
            "Epoch 51:\n",
            "loss: 0.145, accuracy: 93.848\n",
            "val_loss: 0.239, val_accuracy: 91.332\n",
            "-------------------------------------\n",
            "Epoch 52:\n",
            "loss: 0.157, accuracy: 94.541\n",
            "val_loss: 0.246, val_accuracy: 94.986\n",
            "-------------------------------------\n",
            "Epoch 53:\n",
            "loss: 0.131, accuracy: 94.716\n",
            "val_loss: 0.107, val_accuracy: 95.057\n",
            "-------------------------------------\n",
            "Epoch 54:\n",
            "loss: 0.131, accuracy: 94.884\n",
            "val_loss: 0.154, val_accuracy: 94.269\n",
            "-------------------------------------\n",
            "Epoch 55:\n",
            "loss: 0.128, accuracy: 95.489\n",
            "val_loss: 0.102, val_accuracy: 95.559\n",
            "-------------------------------------\n",
            "Epoch 56:\n",
            "loss: 0.164, accuracy: 95.306\n",
            "val_loss: 0.103, val_accuracy: 94.556\n",
            "-------------------------------------\n",
            "Epoch 57:\n",
            "loss: 0.181, accuracy: 94.142\n",
            "val_loss: 0.315, val_accuracy: 91.046\n",
            "-------------------------------------\n",
            "Epoch 58:\n",
            "loss: 0.155, accuracy: 91.536\n",
            "val_loss: 0.267, val_accuracy: 90.544\n",
            "-------------------------------------\n",
            "Epoch 59:\n",
            "loss: 0.127, accuracy: 93.577\n",
            "val_loss: 0.223, val_accuracy: 95.630\n",
            "-------------------------------------\n",
            "Epoch 60:\n",
            "loss: 0.107, accuracy: 95.242\n",
            "val_loss: 0.130, val_accuracy: 95.129\n",
            "-------------------------------------\n",
            "Epoch 61:\n",
            "loss: 0.111, accuracy: 95.242\n",
            "val_loss: 0.136, val_accuracy: 95.344\n",
            "-------------------------------------\n",
            "Epoch 62:\n",
            "loss: 0.088, accuracy: 95.553\n",
            "val_loss: 0.185, val_accuracy: 95.201\n",
            "-------------------------------------\n",
            "Epoch 63:\n",
            "loss: 0.111, accuracy: 96.270\n",
            "val_loss: 0.113, val_accuracy: 96.275\n",
            "-------------------------------------\n",
            "Epoch 64:\n",
            "loss: 0.094, accuracy: 95.776\n",
            "val_loss: 0.100, val_accuracy: 96.562\n",
            "-------------------------------------\n",
            "Epoch 65:\n",
            "loss: 0.075, accuracy: 96.342\n",
            "val_loss: 0.153, val_accuracy: 95.845\n",
            "-------------------------------------\n",
            "Epoch 66:\n",
            "loss: 0.086, accuracy: 96.310\n",
            "val_loss: 0.115, val_accuracy: 96.132\n",
            "-------------------------------------\n",
            "Epoch 67:\n",
            "loss: 0.087, accuracy: 95.840\n",
            "val_loss: 0.202, val_accuracy: 95.272\n",
            "-------------------------------------\n",
            "Epoch 68:\n",
            "loss: 0.092, accuracy: 95.681\n",
            "val_loss: 0.121, val_accuracy: 95.272\n",
            "-------------------------------------\n",
            "Epoch 69:\n",
            "loss: 0.083, accuracy: 96.374\n",
            "val_loss: 0.132, val_accuracy: 95.344\n",
            "-------------------------------------\n",
            "Epoch 70:\n",
            "loss: 0.076, accuracy: 96.063\n",
            "val_loss: 0.122, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 71:\n",
            "loss: 0.147, accuracy: 96.398\n",
            "val_loss: 0.186, val_accuracy: 92.693\n",
            "-------------------------------------\n",
            "Epoch 72:\n",
            "loss: 0.257, accuracy: 91.616\n",
            "val_loss: 0.171, val_accuracy: 93.840\n",
            "-------------------------------------\n",
            "Epoch 73:\n",
            "loss: 0.126, accuracy: 93.609\n",
            "val_loss: 0.129, val_accuracy: 95.415\n",
            "-------------------------------------\n",
            "Epoch 74:\n",
            "loss: 0.081, accuracy: 95.314\n",
            "val_loss: 0.104, val_accuracy: 95.057\n",
            "-------------------------------------\n",
            "Epoch 75:\n",
            "loss: 0.068, accuracy: 95.800\n",
            "val_loss: 0.167, val_accuracy: 96.203\n",
            "-------------------------------------\n",
            "Epoch 76:\n",
            "loss: 0.073, accuracy: 96.358\n",
            "val_loss: 0.072, val_accuracy: 96.705\n",
            "-------------------------------------\n",
            "Epoch 77:\n",
            "loss: 0.099, accuracy: 96.430\n",
            "val_loss: 0.079, val_accuracy: 97.493\n",
            "-------------------------------------\n",
            "Epoch 78:\n",
            "loss: 0.081, accuracy: 96.597\n",
            "val_loss: 0.158, val_accuracy: 96.060\n",
            "-------------------------------------\n",
            "Epoch 79:\n",
            "loss: 0.066, accuracy: 96.741\n",
            "val_loss: 0.084, val_accuracy: 97.206\n",
            "-------------------------------------\n",
            "Epoch 80:\n",
            "loss: 0.063, accuracy: 97.131\n",
            "val_loss: 0.183, val_accuracy: 96.490\n",
            "-------------------------------------\n",
            "Epoch 81:\n",
            "loss: 0.069, accuracy: 97.131\n",
            "val_loss: 0.108, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 82:\n",
            "loss: 0.074, accuracy: 96.948\n",
            "val_loss: 0.131, val_accuracy: 96.562\n",
            "-------------------------------------\n",
            "Epoch 83:\n",
            "loss: 0.075, accuracy: 96.709\n",
            "val_loss: 0.166, val_accuracy: 95.415\n",
            "-------------------------------------\n",
            "Epoch 84:\n",
            "loss: 0.045, accuracy: 97.370\n",
            "val_loss: 0.067, val_accuracy: 96.203\n",
            "-------------------------------------\n",
            "Epoch 85:\n",
            "loss: 0.056, accuracy: 97.665\n",
            "val_loss: 0.058, val_accuracy: 96.991\n",
            "-------------------------------------\n",
            "Epoch 86:\n",
            "loss: 0.078, accuracy: 97.227\n",
            "val_loss: 0.097, val_accuracy: 96.562\n",
            "-------------------------------------\n",
            "Epoch 87:\n",
            "loss: 0.038, accuracy: 96.230\n",
            "val_loss: 0.076, val_accuracy: 96.060\n",
            "-------------------------------------\n",
            "Epoch 88:\n",
            "loss: 0.056, accuracy: 97.019\n",
            "val_loss: 0.102, val_accuracy: 96.633\n",
            "-------------------------------------\n",
            "Epoch 89:\n",
            "loss: 0.037, accuracy: 97.362\n",
            "val_loss: 0.228, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 90:\n",
            "loss: 0.049, accuracy: 97.251\n",
            "val_loss: 0.202, val_accuracy: 97.135\n",
            "-------------------------------------\n",
            "Epoch 91:\n",
            "loss: 0.044, accuracy: 97.681\n",
            "val_loss: 0.226, val_accuracy: 96.920\n",
            "-------------------------------------\n",
            "Epoch 92:\n",
            "loss: 0.037, accuracy: 97.864\n",
            "val_loss: 0.230, val_accuracy: 96.490\n",
            "-------------------------------------\n",
            "Epoch 93:\n",
            "loss: 0.049, accuracy: 97.649\n",
            "val_loss: 0.164, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 94:\n",
            "loss: 0.133, accuracy: 96.183\n",
            "val_loss: 0.275, val_accuracy: 95.702\n",
            "-------------------------------------\n",
            "Epoch 95:\n",
            "loss: 0.803, accuracy: 83.615\n",
            "val_loss: 0.726, val_accuracy: 79.298\n",
            "-------------------------------------\n",
            "Epoch 96:\n",
            "loss: 0.457, accuracy: 81.128\n",
            "val_loss: 0.390, val_accuracy: 89.327\n",
            "-------------------------------------\n",
            "Epoch 97:\n",
            "loss: 0.259, accuracy: 88.819\n",
            "val_loss: 0.241, val_accuracy: 93.195\n",
            "-------------------------------------\n",
            "Epoch 98:\n",
            "loss: 0.174, accuracy: 92.206\n",
            "val_loss: 0.206, val_accuracy: 94.914\n",
            "-------------------------------------\n",
            "Epoch 99:\n",
            "loss: 0.138, accuracy: 94.509\n",
            "val_loss: 0.096, val_accuracy: 96.275\n",
            "-------------------------------------\n",
            "Epoch 100:\n",
            "loss: 0.120, accuracy: 95.457\n",
            "val_loss: 0.073, val_accuracy: 96.132\n",
            "-------------------------------------\n",
            "Epoch 101:\n",
            "loss: 0.085, accuracy: 95.928\n",
            "val_loss: 0.089, val_accuracy: 96.562\n",
            "-------------------------------------\n",
            "Epoch 102:\n",
            "loss: 0.084, accuracy: 95.960\n",
            "val_loss: 0.122, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 103:\n",
            "loss: 0.086, accuracy: 96.007\n",
            "val_loss: 0.099, val_accuracy: 95.845\n",
            "-------------------------------------\n",
            "Epoch 104:\n",
            "loss: 0.074, accuracy: 96.517\n",
            "val_loss: 0.146, val_accuracy: 96.991\n",
            "-------------------------------------\n",
            "Epoch 105:\n",
            "loss: 0.087, accuracy: 96.557\n",
            "val_loss: 0.072, val_accuracy: 97.278\n",
            "-------------------------------------\n",
            "Epoch 106:\n",
            "loss: 0.075, accuracy: 96.613\n",
            "val_loss: 0.098, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 107:\n",
            "loss: 0.064, accuracy: 96.748\n",
            "val_loss: 0.138, val_accuracy: 96.920\n",
            "-------------------------------------\n",
            "Epoch 108:\n",
            "loss: 0.052, accuracy: 96.980\n",
            "val_loss: 0.077, val_accuracy: 97.994\n",
            "-------------------------------------\n",
            "Epoch 109:\n",
            "loss: 0.038, accuracy: 97.139\n",
            "val_loss: 0.082, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 110:\n",
            "loss: 0.069, accuracy: 97.115\n",
            "val_loss: 0.119, val_accuracy: 97.421\n",
            "-------------------------------------\n",
            "Epoch 111:\n",
            "loss: 0.058, accuracy: 97.274\n",
            "val_loss: 0.145, val_accuracy: 96.920\n",
            "-------------------------------------\n",
            "Epoch 112:\n",
            "loss: 0.051, accuracy: 97.354\n",
            "val_loss: 0.114, val_accuracy: 96.991\n",
            "-------------------------------------\n",
            "Epoch 113:\n",
            "loss: 0.053, accuracy: 97.251\n",
            "val_loss: 0.116, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 114:\n",
            "loss: 0.064, accuracy: 97.370\n",
            "val_loss: 0.101, val_accuracy: 97.135\n",
            "-------------------------------------\n",
            "Epoch 115:\n",
            "loss: 0.090, accuracy: 97.195\n",
            "val_loss: 0.096, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 116:\n",
            "loss: 0.063, accuracy: 97.099\n",
            "val_loss: 0.177, val_accuracy: 96.418\n",
            "-------------------------------------\n",
            "Epoch 117:\n",
            "loss: 0.041, accuracy: 97.482\n",
            "val_loss: 0.193, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 118:\n",
            "loss: 0.354, accuracy: 95.107\n",
            "val_loss: 0.339, val_accuracy: 93.481\n",
            "-------------------------------------\n",
            "Epoch 119:\n",
            "loss: 0.121, accuracy: 93.091\n",
            "val_loss: 0.194, val_accuracy: 95.487\n",
            "-------------------------------------\n",
            "Epoch 120:\n",
            "loss: 0.096, accuracy: 95.665\n",
            "val_loss: 0.091, val_accuracy: 97.063\n",
            "-------------------------------------\n",
            "Epoch 121:\n",
            "loss: 0.074, accuracy: 96.342\n",
            "val_loss: 0.061, val_accuracy: 97.421\n",
            "-------------------------------------\n",
            "Epoch 122:\n",
            "loss: 0.068, accuracy: 96.972\n",
            "val_loss: 0.174, val_accuracy: 97.421\n",
            "-------------------------------------\n",
            "Epoch 123:\n",
            "loss: 0.051, accuracy: 97.235\n",
            "val_loss: 0.140, val_accuracy: 97.135\n",
            "-------------------------------------\n",
            "Epoch 124:\n",
            "loss: 0.045, accuracy: 97.418\n",
            "val_loss: 0.104, val_accuracy: 97.564\n",
            "-------------------------------------\n",
            "Epoch 125:\n",
            "loss: 0.036, accuracy: 97.641\n",
            "val_loss: 0.145, val_accuracy: 97.923\n",
            "-------------------------------------\n",
            "Epoch 126:\n",
            "loss: 0.047, accuracy: 97.689\n",
            "val_loss: 0.083, val_accuracy: 96.991\n",
            "-------------------------------------\n",
            "Epoch 127:\n",
            "loss: 0.045, accuracy: 97.649\n",
            "val_loss: 0.068, val_accuracy: 97.636\n",
            "-------------------------------------\n",
            "Epoch 128:\n",
            "loss: 0.045, accuracy: 97.577\n",
            "val_loss: 0.060, val_accuracy: 97.206\n",
            "-------------------------------------\n",
            "Epoch 129:\n",
            "loss: 0.031, accuracy: 97.641\n",
            "val_loss: 0.066, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 130:\n",
            "loss: 0.033, accuracy: 97.713\n",
            "val_loss: 0.192, val_accuracy: 97.063\n",
            "-------------------------------------\n",
            "Epoch 131:\n",
            "loss: 0.037, accuracy: 97.761\n",
            "val_loss: 0.099, val_accuracy: 96.991\n",
            "-------------------------------------\n",
            "Epoch 132:\n",
            "loss: 0.040, accuracy: 97.537\n",
            "val_loss: 0.162, val_accuracy: 97.278\n",
            "-------------------------------------\n",
            "Epoch 133:\n",
            "loss: 0.031, accuracy: 98.000\n",
            "val_loss: 0.195, val_accuracy: 97.206\n",
            "-------------------------------------\n",
            "Epoch 134:\n",
            "loss: 0.042, accuracy: 97.745\n",
            "val_loss: 0.062, val_accuracy: 97.278\n",
            "-------------------------------------\n",
            "Epoch 135:\n",
            "loss: 0.037, accuracy: 97.792\n",
            "val_loss: 0.119, val_accuracy: 97.493\n",
            "-------------------------------------\n",
            "Epoch 136:\n",
            "loss: 0.050, accuracy: 97.633\n",
            "val_loss: 0.147, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 137:\n",
            "loss: 0.045, accuracy: 97.888\n",
            "val_loss: 0.153, val_accuracy: 96.633\n",
            "-------------------------------------\n",
            "Epoch 138:\n",
            "loss: 0.047, accuracy: 97.458\n",
            "val_loss: 0.087, val_accuracy: 97.493\n",
            "-------------------------------------\n",
            "Epoch 139:\n",
            "loss: 0.036, accuracy: 97.721\n",
            "val_loss: 0.054, val_accuracy: 97.063\n",
            "-------------------------------------\n",
            "Epoch 140:\n",
            "loss: 0.032, accuracy: 98.095\n",
            "val_loss: 0.256, val_accuracy: 97.421\n",
            "-------------------------------------\n",
            "Epoch 141:\n",
            "loss: 0.035, accuracy: 97.856\n",
            "val_loss: 0.125, val_accuracy: 97.564\n",
            "-------------------------------------\n",
            "Epoch 142:\n",
            "loss: 0.034, accuracy: 97.968\n",
            "val_loss: 0.270, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 143:\n",
            "loss: 0.019, accuracy: 98.071\n",
            "val_loss: 0.127, val_accuracy: 97.278\n",
            "-------------------------------------\n",
            "Epoch 144:\n",
            "loss: 0.035, accuracy: 97.984\n",
            "val_loss: 0.134, val_accuracy: 97.350\n",
            "-------------------------------------\n",
            "Epoch 145:\n",
            "loss: 0.038, accuracy: 98.000\n",
            "val_loss: 0.318, val_accuracy: 97.063\n",
            "-------------------------------------\n",
            "Epoch 146:\n",
            "loss: 0.049, accuracy: 97.673\n",
            "val_loss: 0.188, val_accuracy: 97.493\n",
            "-------------------------------------\n",
            "Epoch 147:\n",
            "loss: 0.032, accuracy: 97.880\n",
            "val_loss: 0.233, val_accuracy: 95.917\n",
            "-------------------------------------\n",
            "Epoch 148:\n",
            "loss: 0.085, accuracy: 96.764\n",
            "val_loss: 0.139, val_accuracy: 97.135\n",
            "-------------------------------------\n",
            "Epoch 149:\n",
            "loss: 0.038, accuracy: 97.482\n",
            "val_loss: 0.120, val_accuracy: 96.777\n",
            "-------------------------------------\n",
            "Epoch 150:\n",
            "loss: 0.064, accuracy: 97.466\n",
            "val_loss: 0.245, val_accuracy: 97.851\n",
            "-------------------------------------\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8zYG2lFrSPN"
      },
      "source": [
        "# **Test model with new data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tb5fWF_zbml",
        "outputId": "ed721e68-cc19-4250-958b-bc293b7ad828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.set_grad_enabled(False):\n",
        "    for local_batch, local_labels in testloader:\n",
        "        # Transfer to GPU\n",
        "        # local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "        net.eval();\n",
        "        outputs = net(local_batch.float())\n",
        "        # calculate accuracy of predictions in the current batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += local_labels.size(0)\n",
        "        correct += (predicted == local_labels.long()).sum().item()\n",
        "        print('Test accuracy: %.3f' %\n",
        "                (100 * correct / total))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 96.754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIjw8Ku56Rck",
        "outputId": "92e3e86b-3b13-43f2-cda3-8aee2b487eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Label of the classes\n",
        "labels = [    \n",
        "    'WAWING',\n",
        "    'SCISSORS',\n",
        "    'FLIP',\n",
        "    'PUSH&PULL',\n",
        "    'OPEN&CLOSE'\n",
        "] \n",
        "\n",
        "cm = confusion_matrix(local_labels, predicted)\n",
        "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "cm_perc = cm / cm_sum.astype(float) * 100\n",
        "annot = np.empty_like(cm).astype(str)\n",
        "nrows, ncols = cm.shape\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        c = cm[i, j]\n",
        "        p = cm_perc[i, j]\n",
        "        if i == j:\n",
        "            s = cm_sum[i]\n",
        "            annot[i, j] = '%.2f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "            annot[i, j] = ''\n",
        "        else:\n",
        "            annot[i, j] = '%.2f%%\\n%d' % (p, c)\n",
        "cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "cm.index.name = 'ACTUAL'\n",
        "cm.columns.name = 'PREDICTED'\n",
        "plt.subplots(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=annot, fmt='', cmap=\"YlGnBu\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f739ee8c0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nJs3Xm6E1DC"
      },
      "source": [
        "# **Save model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmQ0sUKfUuJE"
      },
      "source": [
        "# Load a sample image\n",
        "example_image, example_label = next(iter(trainloader))\n",
        "# Run the tracing\n",
        "traced_script_module = torch.jit.trace(net, example_image.float())\n",
        "# Save the converted model\n",
        "traced_script_module.save('/content/drive/My Drive/MastersThesis/Pytorch/model.pt')"
      ],
      "execution_count": 64,
      "outputs": []
    }
  ]
}