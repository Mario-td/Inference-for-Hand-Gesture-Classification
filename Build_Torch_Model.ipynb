{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build Torch Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0Rna9jqAntd3wBOJp3JJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mario-td/Inference-for-Hand-Gesture-Classification/blob/master/Build_Torch_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JYDptYxAzfb"
      },
      "source": [
        "# **Load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84pJ5youlPu7"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSlHF1JM4LaC",
        "outputId": "74ad2fcf-031b-498c-d78f-3363adfd91cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOOubOgKhR93",
        "outputId": "90d2542d-53f9-4bf6-83fd-159b08c3dd3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "# Path to the files\n",
        "path = '/content/drive/My Drive/MastersThesis/Dataset'\n",
        "\n",
        "training = pd.read_csv(path + '/TrainingSet2D.csv')\n",
        "test = pd.read_csv(path + '/TestSet2D.csv')\n",
        "\n",
        "# Data visualization\n",
        "training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Sequence</th>\n",
              "      <th>j0_x</th>\n",
              "      <th>j0_y</th>\n",
              "      <th>j1_x</th>\n",
              "      <th>j1_y</th>\n",
              "      <th>j2_x</th>\n",
              "      <th>j2_y</th>\n",
              "      <th>j3_x</th>\n",
              "      <th>j3_y</th>\n",
              "      <th>j4_x</th>\n",
              "      <th>j4_y</th>\n",
              "      <th>j5_x</th>\n",
              "      <th>j5_y</th>\n",
              "      <th>j6_x</th>\n",
              "      <th>j6_y</th>\n",
              "      <th>j7_x</th>\n",
              "      <th>j7_y</th>\n",
              "      <th>j8_x</th>\n",
              "      <th>j8_y</th>\n",
              "      <th>j9_x</th>\n",
              "      <th>j9_y</th>\n",
              "      <th>j10_x</th>\n",
              "      <th>j10_y</th>\n",
              "      <th>j11_x</th>\n",
              "      <th>j11_y</th>\n",
              "      <th>j12_x</th>\n",
              "      <th>j12_y</th>\n",
              "      <th>j13_x</th>\n",
              "      <th>j13_y</th>\n",
              "      <th>j14_x</th>\n",
              "      <th>j14_y</th>\n",
              "      <th>j15_x</th>\n",
              "      <th>j15_y</th>\n",
              "      <th>j16_x</th>\n",
              "      <th>j16_y</th>\n",
              "      <th>j17_x</th>\n",
              "      <th>j17_y</th>\n",
              "      <th>j18_x</th>\n",
              "      <th>j18_y</th>\n",
              "      <th>j19_x</th>\n",
              "      <th>j19_y</th>\n",
              "      <th>j20_x</th>\n",
              "      <th>j20_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427588</td>\n",
              "      <td>0.782585</td>\n",
              "      <td>0.487573</td>\n",
              "      <td>0.679753</td>\n",
              "      <td>0.538989</td>\n",
              "      <td>0.599772</td>\n",
              "      <td>0.564697</td>\n",
              "      <td>0.508366</td>\n",
              "      <td>0.607544</td>\n",
              "      <td>0.462663</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.474089</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.394108</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.348405</td>\n",
              "      <td>0.453296</td>\n",
              "      <td>0.291276</td>\n",
              "      <td>0.410449</td>\n",
              "      <td>0.474089</td>\n",
              "      <td>0.401880</td>\n",
              "      <td>0.405534</td>\n",
              "      <td>0.401880</td>\n",
              "      <td>0.336979</td>\n",
              "      <td>0.393311</td>\n",
              "      <td>0.268424</td>\n",
              "      <td>0.367603</td>\n",
              "      <td>0.496940</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.439811</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.371257</td>\n",
              "      <td>0.350464</td>\n",
              "      <td>0.314128</td>\n",
              "      <td>0.341895</td>\n",
              "      <td>0.565495</td>\n",
              "      <td>0.307617</td>\n",
              "      <td>0.519792</td>\n",
              "      <td>0.299048</td>\n",
              "      <td>0.474089</td>\n",
              "      <td>0.307617</td>\n",
              "      <td>0.416960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.376929</td>\n",
              "      <td>0.779590</td>\n",
              "      <td>0.444702</td>\n",
              "      <td>0.666634</td>\n",
              "      <td>0.470117</td>\n",
              "      <td>0.553678</td>\n",
              "      <td>0.487061</td>\n",
              "      <td>0.474609</td>\n",
              "      <td>0.520947</td>\n",
              "      <td>0.429427</td>\n",
              "      <td>0.368457</td>\n",
              "      <td>0.452018</td>\n",
              "      <td>0.368457</td>\n",
              "      <td>0.384245</td>\n",
              "      <td>0.359985</td>\n",
              "      <td>0.339062</td>\n",
              "      <td>0.351514</td>\n",
              "      <td>0.282585</td>\n",
              "      <td>0.326099</td>\n",
              "      <td>0.474609</td>\n",
              "      <td>0.317627</td>\n",
              "      <td>0.395540</td>\n",
              "      <td>0.300684</td>\n",
              "      <td>0.327767</td>\n",
              "      <td>0.283740</td>\n",
              "      <td>0.271289</td>\n",
              "      <td>0.292212</td>\n",
              "      <td>0.508496</td>\n",
              "      <td>0.266797</td>\n",
              "      <td>0.429427</td>\n",
              "      <td>0.249854</td>\n",
              "      <td>0.372949</td>\n",
              "      <td>0.249854</td>\n",
              "      <td>0.316471</td>\n",
              "      <td>0.275269</td>\n",
              "      <td>0.564974</td>\n",
              "      <td>0.215967</td>\n",
              "      <td>0.519792</td>\n",
              "      <td>0.207495</td>\n",
              "      <td>0.474609</td>\n",
              "      <td>0.207495</td>\n",
              "      <td>0.440723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.364795</td>\n",
              "      <td>0.769401</td>\n",
              "      <td>0.416064</td>\n",
              "      <td>0.666862</td>\n",
              "      <td>0.450244</td>\n",
              "      <td>0.552930</td>\n",
              "      <td>0.467334</td>\n",
              "      <td>0.473177</td>\n",
              "      <td>0.492969</td>\n",
              "      <td>0.427604</td>\n",
              "      <td>0.347705</td>\n",
              "      <td>0.450391</td>\n",
              "      <td>0.339160</td>\n",
              "      <td>0.393424</td>\n",
              "      <td>0.330615</td>\n",
              "      <td>0.336458</td>\n",
              "      <td>0.313525</td>\n",
              "      <td>0.279492</td>\n",
              "      <td>0.304980</td>\n",
              "      <td>0.473177</td>\n",
              "      <td>0.296436</td>\n",
              "      <td>0.393424</td>\n",
              "      <td>0.270801</td>\n",
              "      <td>0.336458</td>\n",
              "      <td>0.253711</td>\n",
              "      <td>0.268099</td>\n",
              "      <td>0.270801</td>\n",
              "      <td>0.507357</td>\n",
              "      <td>0.236621</td>\n",
              "      <td>0.438997</td>\n",
              "      <td>0.219531</td>\n",
              "      <td>0.382031</td>\n",
              "      <td>0.210986</td>\n",
              "      <td>0.325065</td>\n",
              "      <td>0.253711</td>\n",
              "      <td>0.564323</td>\n",
              "      <td>0.202441</td>\n",
              "      <td>0.530143</td>\n",
              "      <td>0.185352</td>\n",
              "      <td>0.484570</td>\n",
              "      <td>0.176807</td>\n",
              "      <td>0.438997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.349219</td>\n",
              "      <td>0.747461</td>\n",
              "      <td>0.400049</td>\n",
              "      <td>0.657096</td>\n",
              "      <td>0.442407</td>\n",
              "      <td>0.555436</td>\n",
              "      <td>0.467822</td>\n",
              "      <td>0.476367</td>\n",
              "      <td>0.484766</td>\n",
              "      <td>0.408594</td>\n",
              "      <td>0.340747</td>\n",
              "      <td>0.465072</td>\n",
              "      <td>0.323804</td>\n",
              "      <td>0.386003</td>\n",
              "      <td>0.306860</td>\n",
              "      <td>0.329525</td>\n",
              "      <td>0.289917</td>\n",
              "      <td>0.284342</td>\n",
              "      <td>0.289917</td>\n",
              "      <td>0.476367</td>\n",
              "      <td>0.264502</td>\n",
              "      <td>0.397298</td>\n",
              "      <td>0.247559</td>\n",
              "      <td>0.329525</td>\n",
              "      <td>0.222144</td>\n",
              "      <td>0.273047</td>\n",
              "      <td>0.256030</td>\n",
              "      <td>0.510254</td>\n",
              "      <td>0.222144</td>\n",
              "      <td>0.431185</td>\n",
              "      <td>0.205200</td>\n",
              "      <td>0.374707</td>\n",
              "      <td>0.196729</td>\n",
              "      <td>0.329525</td>\n",
              "      <td>0.239087</td>\n",
              "      <td>0.566732</td>\n",
              "      <td>0.188257</td>\n",
              "      <td>0.532845</td>\n",
              "      <td>0.171313</td>\n",
              "      <td>0.498958</td>\n",
              "      <td>0.162842</td>\n",
              "      <td>0.453776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.342187</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.393750</td>\n",
              "      <td>0.646875</td>\n",
              "      <td>0.428125</td>\n",
              "      <td>0.555208</td>\n",
              "      <td>0.436719</td>\n",
              "      <td>0.463542</td>\n",
              "      <td>0.453906</td>\n",
              "      <td>0.406250</td>\n",
              "      <td>0.316406</td>\n",
              "      <td>0.463542</td>\n",
              "      <td>0.299219</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.282031</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.264844</td>\n",
              "      <td>0.280208</td>\n",
              "      <td>0.273438</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.247656</td>\n",
              "      <td>0.394792</td>\n",
              "      <td>0.221875</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.196094</td>\n",
              "      <td>0.280208</td>\n",
              "      <td>0.239063</td>\n",
              "      <td>0.509375</td>\n",
              "      <td>0.204687</td>\n",
              "      <td>0.440625</td>\n",
              "      <td>0.178906</td>\n",
              "      <td>0.383333</td>\n",
              "      <td>0.161719</td>\n",
              "      <td>0.326042</td>\n",
              "      <td>0.221875</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.178906</td>\n",
              "      <td>0.532292</td>\n",
              "      <td>0.153125</td>\n",
              "      <td>0.497917</td>\n",
              "      <td>0.135937</td>\n",
              "      <td>0.452083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111547</th>\n",
              "      <td>4</td>\n",
              "      <td>27</td>\n",
              "      <td>0.594629</td>\n",
              "      <td>0.701237</td>\n",
              "      <td>0.634863</td>\n",
              "      <td>0.687826</td>\n",
              "      <td>0.670068</td>\n",
              "      <td>0.654297</td>\n",
              "      <td>0.685156</td>\n",
              "      <td>0.620768</td>\n",
              "      <td>0.675098</td>\n",
              "      <td>0.587240</td>\n",
              "      <td>0.624805</td>\n",
              "      <td>0.526888</td>\n",
              "      <td>0.619775</td>\n",
              "      <td>0.493359</td>\n",
              "      <td>0.624805</td>\n",
              "      <td>0.466536</td>\n",
              "      <td>0.609717</td>\n",
              "      <td>0.439714</td>\n",
              "      <td>0.584570</td>\n",
              "      <td>0.526888</td>\n",
              "      <td>0.579541</td>\n",
              "      <td>0.493359</td>\n",
              "      <td>0.569482</td>\n",
              "      <td>0.479948</td>\n",
              "      <td>0.559424</td>\n",
              "      <td>0.446419</td>\n",
              "      <td>0.564453</td>\n",
              "      <td>0.547005</td>\n",
              "      <td>0.544336</td>\n",
              "      <td>0.526888</td>\n",
              "      <td>0.534277</td>\n",
              "      <td>0.506771</td>\n",
              "      <td>0.524219</td>\n",
              "      <td>0.506771</td>\n",
              "      <td>0.529248</td>\n",
              "      <td>0.587240</td>\n",
              "      <td>0.524219</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.519189</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.519189</td>\n",
              "      <td>0.560417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111548</th>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>0.605029</td>\n",
              "      <td>0.717969</td>\n",
              "      <td>0.634570</td>\n",
              "      <td>0.686458</td>\n",
              "      <td>0.670020</td>\n",
              "      <td>0.662826</td>\n",
              "      <td>0.681836</td>\n",
              "      <td>0.623437</td>\n",
              "      <td>0.687744</td>\n",
              "      <td>0.599805</td>\n",
              "      <td>0.616846</td>\n",
              "      <td>0.536784</td>\n",
              "      <td>0.616846</td>\n",
              "      <td>0.481641</td>\n",
              "      <td>0.616846</td>\n",
              "      <td>0.450130</td>\n",
              "      <td>0.610938</td>\n",
              "      <td>0.410742</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>0.528906</td>\n",
              "      <td>0.569580</td>\n",
              "      <td>0.473763</td>\n",
              "      <td>0.557764</td>\n",
              "      <td>0.434375</td>\n",
              "      <td>0.551855</td>\n",
              "      <td>0.402865</td>\n",
              "      <td>0.551855</td>\n",
              "      <td>0.552539</td>\n",
              "      <td>0.534131</td>\n",
              "      <td>0.513151</td>\n",
              "      <td>0.522314</td>\n",
              "      <td>0.489518</td>\n",
              "      <td>0.504590</td>\n",
              "      <td>0.458008</td>\n",
              "      <td>0.528223</td>\n",
              "      <td>0.584049</td>\n",
              "      <td>0.516406</td>\n",
              "      <td>0.568294</td>\n",
              "      <td>0.498682</td>\n",
              "      <td>0.552539</td>\n",
              "      <td>0.492773</td>\n",
              "      <td>0.528906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111549</th>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>0.602441</td>\n",
              "      <td>0.715365</td>\n",
              "      <td>0.631494</td>\n",
              "      <td>0.699870</td>\n",
              "      <td>0.666357</td>\n",
              "      <td>0.668880</td>\n",
              "      <td>0.677979</td>\n",
              "      <td>0.637891</td>\n",
              "      <td>0.689600</td>\n",
              "      <td>0.606901</td>\n",
              "      <td>0.619873</td>\n",
              "      <td>0.537174</td>\n",
              "      <td>0.619873</td>\n",
              "      <td>0.475195</td>\n",
              "      <td>0.619873</td>\n",
              "      <td>0.444206</td>\n",
              "      <td>0.614062</td>\n",
              "      <td>0.405469</td>\n",
              "      <td>0.579199</td>\n",
              "      <td>0.529427</td>\n",
              "      <td>0.567578</td>\n",
              "      <td>0.475195</td>\n",
              "      <td>0.555957</td>\n",
              "      <td>0.428711</td>\n",
              "      <td>0.550146</td>\n",
              "      <td>0.397721</td>\n",
              "      <td>0.550146</td>\n",
              "      <td>0.552669</td>\n",
              "      <td>0.532715</td>\n",
              "      <td>0.513932</td>\n",
              "      <td>0.515283</td>\n",
              "      <td>0.475195</td>\n",
              "      <td>0.503662</td>\n",
              "      <td>0.451953</td>\n",
              "      <td>0.526904</td>\n",
              "      <td>0.583659</td>\n",
              "      <td>0.509473</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.492041</td>\n",
              "      <td>0.544922</td>\n",
              "      <td>0.486230</td>\n",
              "      <td>0.521680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111550</th>\n",
              "      <td>4</td>\n",
              "      <td>30</td>\n",
              "      <td>0.601611</td>\n",
              "      <td>0.717057</td>\n",
              "      <td>0.630786</td>\n",
              "      <td>0.693717</td>\n",
              "      <td>0.665796</td>\n",
              "      <td>0.670378</td>\n",
              "      <td>0.683301</td>\n",
              "      <td>0.639258</td>\n",
              "      <td>0.689136</td>\n",
              "      <td>0.608138</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.538118</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.491439</td>\n",
              "      <td>0.619116</td>\n",
              "      <td>0.444759</td>\n",
              "      <td>0.613281</td>\n",
              "      <td>0.405859</td>\n",
              "      <td>0.578271</td>\n",
              "      <td>0.530339</td>\n",
              "      <td>0.572437</td>\n",
              "      <td>0.475879</td>\n",
              "      <td>0.560767</td>\n",
              "      <td>0.436979</td>\n",
              "      <td>0.549097</td>\n",
              "      <td>0.398079</td>\n",
              "      <td>0.549097</td>\n",
              "      <td>0.553678</td>\n",
              "      <td>0.537427</td>\n",
              "      <td>0.506999</td>\n",
              "      <td>0.525757</td>\n",
              "      <td>0.475879</td>\n",
              "      <td>0.508252</td>\n",
              "      <td>0.444759</td>\n",
              "      <td>0.525757</td>\n",
              "      <td>0.592578</td>\n",
              "      <td>0.508252</td>\n",
              "      <td>0.561458</td>\n",
              "      <td>0.490747</td>\n",
              "      <td>0.538118</td>\n",
              "      <td>0.479077</td>\n",
              "      <td>0.514779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111551</th>\n",
              "      <td>4</td>\n",
              "      <td>31</td>\n",
              "      <td>0.596777</td>\n",
              "      <td>0.721745</td>\n",
              "      <td>0.625586</td>\n",
              "      <td>0.698698</td>\n",
              "      <td>0.665918</td>\n",
              "      <td>0.675651</td>\n",
              "      <td>0.677441</td>\n",
              "      <td>0.637240</td>\n",
              "      <td>0.688965</td>\n",
              "      <td>0.606510</td>\n",
              "      <td>0.619824</td>\n",
              "      <td>0.545052</td>\n",
              "      <td>0.619824</td>\n",
              "      <td>0.491276</td>\n",
              "      <td>0.619824</td>\n",
              "      <td>0.452865</td>\n",
              "      <td>0.614062</td>\n",
              "      <td>0.414453</td>\n",
              "      <td>0.585254</td>\n",
              "      <td>0.529687</td>\n",
              "      <td>0.573730</td>\n",
              "      <td>0.475911</td>\n",
              "      <td>0.562207</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.556445</td>\n",
              "      <td>0.399089</td>\n",
              "      <td>0.550684</td>\n",
              "      <td>0.552734</td>\n",
              "      <td>0.539160</td>\n",
              "      <td>0.514323</td>\n",
              "      <td>0.527637</td>\n",
              "      <td>0.475911</td>\n",
              "      <td>0.516113</td>\n",
              "      <td>0.445182</td>\n",
              "      <td>0.527637</td>\n",
              "      <td>0.591146</td>\n",
              "      <td>0.510352</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.493066</td>\n",
              "      <td>0.537370</td>\n",
              "      <td>0.481543</td>\n",
              "      <td>0.514323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>111552 rows Ã— 44 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Label  Sequence      j0_x  ...     j19_y     j20_x     j20_y\n",
              "0           0         0  0.427588  ...  0.474089  0.307617  0.416960\n",
              "1           0         1  0.376929  ...  0.474609  0.207495  0.440723\n",
              "2           0         2  0.364795  ...  0.484570  0.176807  0.438997\n",
              "3           0         3  0.349219  ...  0.498958  0.162842  0.453776\n",
              "4           0         4  0.342187  ...  0.497917  0.135937  0.452083\n",
              "...       ...       ...       ...  ...       ...       ...       ...\n",
              "111547      4        27  0.594629  ...  0.560417  0.519189  0.560417\n",
              "111548      4        28  0.605029  ...  0.552539  0.492773  0.528906\n",
              "111549      4        29  0.602441  ...  0.544922  0.486230  0.521680\n",
              "111550      4        30  0.601611  ...  0.538118  0.479077  0.514779\n",
              "111551      4        31  0.596777  ...  0.537370  0.481543  0.514323\n",
              "\n",
              "[111552 rows x 44 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkobkHgiH0ok",
        "outputId": "a12b7000-4365-4f4f-db21-e501b65211f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''pd.options.mode.chained_assignment = None \n",
        "\n",
        "# Remove class\n",
        "gest_class = 0\n",
        "\n",
        "training = training[training['Label'] != gest_class]\n",
        "test = test[test['Label'] != gest_class]\n",
        "\n",
        "# Adjust the label to start from 0\n",
        "training['Label'] -= 1\n",
        "test['Label'] -= 1\n",
        "\n",
        "training'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"pd.options.mode.chained_assignment = None \\n\\n# Remove class\\ngest_class = 0\\n\\ntraining = training[training['Label'] != gest_class]\\ntest = test[test['Label'] != gest_class]\\n\\n# Adjust the label to start from 0\\ntraining['Label'] -= 1\\ntest['Label'] -= 1\\n\\ntraining\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-zAt-V0L3fw"
      },
      "source": [
        "n_classes = len(training['Label'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnRoqinhq0F",
        "outputId": "066a32de-a7aa-493e-f003-68fe9821a900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the number of frames per sample\n",
        "t_steps = max(training['Sequence']) + 1\n",
        "t_steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSdg6PSbhyXE"
      },
      "source": [
        "# Get the labels\n",
        "Y_train = np.array(training['Label'][0::t_steps][:], dtype=np.int32)\n",
        "Y_test = np.array(test['Label'][0::t_steps][:], dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0842ivyhyeL"
      },
      "source": [
        "# Get the data\n",
        "X_train = pd.DataFrame.to_numpy(training[training.columns[2:]])\n",
        "X_test = pd.DataFrame.to_numpy(test[test.columns[2:]])\n",
        "# Split into the number of samples\n",
        "X_train = np.array(np.split(X_train, Y_train.size))\n",
        "X_test = np.array(np.split(X_test, Y_test.size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m2jTkBPhyji"
      },
      "source": [
        "# Split into train, validation \n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcMIch7mE1M",
        "outputId": "a41f3ecb-4215-47f0-cbe1-e59d0a3eb494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('X_train shape '+ str(X_train.shape))\n",
        "print('X_val shape '+ str(X_val.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape (3137, 32, 42)\n",
            "X_val shape (349, 32, 42)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aXj-yR0igwD"
      },
      "source": [
        "# **Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgyjClybkdn-"
      },
      "source": [
        "def dataAugmentation(X_, Y, c):\n",
        "    X = X_[Y == c]\n",
        "    # Duplicate the samples by inverting the order of the sequence in time\n",
        "    X_inv_t = [X[i][::-1] for i in range(len(X))]\n",
        "    X = np.append(X, X_inv_t, 0)\n",
        "    # Duplicate the samples by inverting the x coordinate\n",
        "    # with respect to the y axis (mirroring)\n",
        "    X_inv_pos = []\n",
        "    for k in range(len(X)):\n",
        "        Xj = []\n",
        "        for j in range(len(X[k])):\n",
        "            Xi = []\n",
        "            for i in range(len(X[k][j])):\n",
        "                if i % 3 == 0:\n",
        "                    Xi.append(abs(X[k][j][i] - 1))\n",
        "                else:\n",
        "                    Xi.append(X[k][j][i])\n",
        "            Xj.append(Xi)\n",
        "        X_inv_pos.append(Xj)\n",
        "    X = np.append(X, X_inv_pos, 0)\n",
        "    X_ = np.append(X_[Y != c], X, 0)\n",
        "    Y = np.append(Y[Y != c], np.ones(len(X)) * c)\n",
        "    return X_, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvCD0tbtifqq"
      },
      "source": [
        "# Duplicate samples of all classes in training, validation and test sets\n",
        "# separately to avoid overfitting mixing samples\n",
        "for c in range(n_classes):\n",
        "    #X_train, Y_train = dataAugmentation(X_train, Y_train, c)\n",
        "    X_val, Y_val = dataAugmentation(X_val, Y_val, c)\n",
        "    X_test, Y_test = dataAugmentation(X_test, Y_test, c)\n",
        "\n",
        "X_train, Y_train = dataAugmentation(X_train, Y_train, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY9GJc1lpI1w",
        "outputId": "a02aa72b-6b1a-472b-a07f-135126e3455e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prints the number of samples per class in the training and test set\n",
        "for i in range(n_classes):\n",
        "    print('train samples %d: ' % i + str(np.count_nonzero(Y_train == i)))\n",
        "print('----------------------')\n",
        "for i in range(n_classes):\n",
        "    print('val samples %d: ' % i + str(np.count_nonzero(Y_val == i)))\n",
        "print('----------------------')\n",
        "for i in range(n_classes):\n",
        "    print('test samples %d: ' % i + str(np.count_nonzero(Y_test == i)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train samples 0: 2524\n",
            "train samples 1: 608\n",
            "train samples 2: 620\n",
            "train samples 3: 616\n",
            "train samples 4: 662\n",
            "----------------------\n",
            "val samples 0: 324\n",
            "val samples 1: 276\n",
            "val samples 2: 268\n",
            "val samples 3: 224\n",
            "val samples 4: 304\n",
            "----------------------\n",
            "test samples 0: 288\n",
            "test samples 1: 284\n",
            "test samples 2: 300\n",
            "test samples 3: 288\n",
            "test samples 4: 288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0I9Yz2LAAqS"
      },
      "source": [
        "# Convert to tensor\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_val = torch.from_numpy(X_val)\n",
        "X_test = torch.from_numpy(X_test) \n",
        "Y_train = torch.from_numpy(Y_train)\n",
        "Y_val = torch.from_numpy(Y_val)\n",
        "Y_test = torch.from_numpy(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFHMGVv2UrDe",
        "outputId": "441f3689-c687-480b-8e24-e2c47235a807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5030])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUNuVbCyUTRB",
        "outputId": "6bcf52bd-6df9-453b-8288-dc95a1247f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train = X_train.transpose(1,2)\n",
        "X_val = X_val.transpose(1,2)\n",
        "X_test = X_test.transpose(1,2)\n",
        "X_train[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([42, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9iJtretAyE7"
      },
      "source": [
        "# **Build Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvNu26noqu9R",
        "outputId": "f4c0a0a1-2782-44b9-b450-16037f805241",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.conv1 = nn.Conv1d(in_channels=42, out_channels=64, kernel_size=3, stride=1)\n",
        "      self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "      self.flatten = nn.Flatten(start_dim=1, end_dim=2)\n",
        "      self.lstm1 = torch.nn.LSTM(\n",
        "            input_size= 64,\n",
        "            hidden_size=200,\n",
        "            batch_first=True,\n",
        "            num_layers=1)\n",
        "      self.lin = nn.Linear(5600, n_classes)\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      self.softmax = nn.LogSoftmax(dim = 1)\n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = x.view(-1, 28, 64)\n",
        "      x, _ = self.lstm1(x)\n",
        "      x = x.contiguous().view(-1, 5600)\n",
        "      x = self.lin(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "net = Net()\n",
        "print(net)\n",
        "output = net(X_train.float())\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv1d(42, 64, kernel_size=(3,), stride=(1,))\n",
            "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
            "  (flatten): Flatten()\n",
            "  (lstm1): LSTM(64, 200, batch_first=True)\n",
            "  (lin): Linear(in_features=5600, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "torch.Size([5030, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHEEqUrnqFgt"
      },
      "source": [
        "learning_rate = 0.0011\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "crossentropy = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHmqu3g6Ak26"
      },
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Parameters\n",
        "params = {'batch_size': 1064,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-ZEbwE4wgCo"
      },
      "source": [
        "train_data = []\n",
        "for i in range(len(X_train)):\n",
        "   train_data.append([X_train[i], Y_train[i]])\n",
        "val_data = []\n",
        "for i in range(len(X_val)):\n",
        "   val_data.append([X_val[i], Y_val[i]])\n",
        "test_data = []\n",
        "for i in range(len(X_test)):\n",
        "   test_data.append([X_test[i], Y_test[i]])\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, **params)\n",
        "validationloader = torch.utils.data.DataLoader(val_data, **params)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSP15MaECTrK",
        "outputId": "3616baa3-d24c-43b5-8a6d-424c37814242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for epoch in range(150):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        local_batch, local_labels = data\n",
        "        # Transfer to GPU\n",
        "        # local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "        # Switch model to training mode, clear gradient accumulators\n",
        "        net.train(); optimizer.zero_grad()\n",
        "        # Forward + Backward + Optimize\n",
        "        outputs = net(local_batch.float())\n",
        "        loss = crossentropy(outputs, local_labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy of predictions in the current batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += local_labels.size(0)\n",
        "        correct += (predicted == local_labels.long()).sum().item()\n",
        "        # Print statistics\n",
        "        print('[%d, %5d] loss: %.3f, accuracy: %.3f' %\n",
        "                    (epoch + 1, i + 1, loss.item(), 100 * correct/total))\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for local_batch, local_labels in validationloader:\n",
        "            # Transfer to GPU\n",
        "            # local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "            net.eval();\n",
        "            outputs = net(local_batch.float())\n",
        "            loss = crossentropy(outputs, local_labels.long())\n",
        "            # Calculate accuracy of predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += local_labels.size(0)\n",
        "            correct += (predicted == local_labels.long()).sum().item()\n",
        "            print('           val_loss: %.3f, val_accuracy: %.3f' %\n",
        "                    (loss.item(), 100 * correct/total))\n",
        "            \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,     1] loss: 1.600, accuracy: 46.898\n",
            "[1,     2] loss: 1.519, accuracy: 47.744\n",
            "[1,     3] loss: 1.462, accuracy: 47.932\n",
            "[1,     4] loss: 1.424, accuracy: 48.285\n",
            "[1,     5] loss: 1.371, accuracy: 49.046\n",
            "           val_loss: 1.856, val_accuracy: 23.402\n",
            "           val_loss: 1.866, val_accuracy: 23.209\n",
            "[2,     1] loss: 1.414, accuracy: 50.564\n",
            "[2,     2] loss: 1.390, accuracy: 50.658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-67aed0b8df14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8zYG2lFrSPN"
      },
      "source": [
        "# **Test model with new data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tb5fWF_zbml"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.set_grad_enabled(False):\n",
        "        for local_batch, local_labels in testloader:\n",
        "            # Transfer to GPU\n",
        "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "            net.eval();\n",
        "            outputs = net(local_batch.float())\n",
        "            loss = crossentropy(outputs, local_labels.long())\n",
        "            # calculate accuracy of predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += local_labels.size(0)\n",
        "            correct += (predicted == local_labels.long()).sum().item()\n",
        "            print('           test_loss: %.3f, test_accuracy: %.3f' %\n",
        "                    (loss.item(), 100 * correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIjw8Ku56Rck"
      },
      "source": [
        "\n",
        "# Label of the classes\n",
        "LABELS = [    \n",
        "    #'WAWING',\n",
        "    'SCISSORS',\n",
        "    'FLIP',\n",
        "    'PUSH&PULL',\n",
        "    'OPEN&CLOSE'\n",
        "] \n",
        "\n",
        "cm = confusion_matrix(local_labels, predicted)\n",
        "plt.figure(figsize = (7, 5))\n",
        "sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS, cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bpM9auBy579"
      },
      "source": [
        "# Functions for loading the data and labels\n",
        "def load_data(path, steps):\n",
        "    file = open(path, 'r')\n",
        "    data = []\n",
        "    for line in file.readlines():\n",
        "        data.append([float(i) for i in line.split(',') if i.strip()]) \n",
        "    file.close()\n",
        "\n",
        "    # Subdivide the list by the number of steps per sample\n",
        "    data = [data[x:x+steps] for x in range(0, len(data), steps)]\n",
        "    return data \n",
        "    \n",
        "def load_label(path):\n",
        "    file = open(path, 'r')\n",
        "    label = []\n",
        "    for line in file.readlines():\n",
        "        for i in line.split():\n",
        "            l = int(i) - 1 # -1 for indexing from 0\n",
        "        label.append(l) \n",
        "    file.close()\n",
        "    return label "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zctsaqtpzYjI"
      },
      "source": [
        "newData = load_data('/content/drive/My Drive/MastersThesis/GithubHandGesture/newGestures/1_2D.txt',32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXj_g6kL9xuf"
      },
      "source": [
        "newData = torch.Tensor(newData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53pU7ptz-EUQ"
      },
      "source": [
        "newData = newData.transpose(1,2)\n",
        "newData.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHlAsSwc_Ao9"
      },
      "source": [
        "newData[1][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdkcP3df-M0w"
      },
      "source": [
        "outputs = net(newData.float())\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nJs3Xm6E1DC"
      },
      "source": [
        "# **Save model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmQ0sUKfUuJE"
      },
      "source": [
        "# Load a sample image\n",
        "example_image, example_label = next(iter(trainloader))\n",
        "# Run the tracing\n",
        "traced_script_module = torch.jit.trace(net, example_image.float())\n",
        "# Save the converted model\n",
        "traced_script_module.save('/content/drive/My Drive/MastersThesis/Pytorch/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgSbOYPSRo8H"
      },
      "source": [
        "model = torch.jit.load('/content/drive/My Drive/MastersThesis/Pytorch/model.pt')\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfEG0EBTC57T"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.set_grad_enabled(False):\n",
        "        for local_batch, local_labels in testloader:\n",
        "            # Transfer to GPU\n",
        "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
        "            net.eval();\n",
        "            outputs = model(local_batch.float())\n",
        "            loss = crossentropy(outputs, local_labels.long())\n",
        "            # calculate accuracy of predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += local_labels.size(0)\n",
        "            correct += (predicted == local_labels.long()).sum().item()\n",
        "            print('           test_loss: %.3f, test_accuracy: %.3f' %\n",
        "                    (loss.item(), 100 * correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}